# Alerting rules for LabelMint
# ============================

groups:
  # Infrastructure alerts
  - name: infrastructure
    rules:
      # High CPU usage
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: High CPU usage on {{ $labels.instance }}
          description: "CPU usage is above 80% for more than 5 minutes on {{ $labels.instance }}"

      # High memory usage
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: High memory usage on {{ $labels.instance }}
          description: "Memory usage is above 85% for more than 5 minutes on {{ $labels.instance }}"

      # Disk space low
      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: Disk space low on {{ $labels.instance }}
          description: "Disk usage is above 85% for more than 10 minutes on {{ $labels.device }}"

      # Node down
      - alert: NodeDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: Node {{ $labels.instance }} is down
          description: "Node {{ $labels.instance }} has been down for more than 1 minute"

  # Application alerts
  - name: applications
    rules:
      # Web application down
      - alert: WebApplicationDown
        expr: up{job="labelmint-web"} == 0
        for: 1m
        labels:
          severity: critical
          team: frontend
        annotations:
          summary: Web application is down
          description: "The web application has been down for more than 1 minute"

      # Backend API down
      - alert: BackendAPIDown
        expr: up{job="labelmint-labeling-backend"} == 0
        for: 1m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: Backend API is down
          description: "The backend API has been down for more than 1 minute"

      # Payment API down
      - alert: PaymentAPIDown
        expr: up{job="labelmint-payment-backend"} == 0
        for: 1m
        labels:
          severity: critical
          team: payments
        annotations:
          summary: Payment API is down
          description: "The payment API has been down for more than 1 minute"

      # High error rate
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100 > 5
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: High error rate
          description: "Error rate is above 5% for more than 5 minutes"

      # High response time
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: High response time
          description: "95th percentile response time is above 1 second for more than 5 minutes"

      # Database connections high
      - alert: DatabaseConnectionsHigh
        expr: pg_stat_activity_count > 80
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: High database connections
          description: "Database has more than 80 active connections"

  # Database alerts
  - name: database
    rules:
      # PostgreSQL down
      - alert: PostgreSQLDown
        expr: up{job="postgres-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: PostgreSQL is down
          description: "PostgreSQL has been down for more than 1 minute"

      # PostgreSQL replication lag
      - alert: PostgreSQLReplicationLag
        expr: pg_replication_lag_seconds > 30
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: PostgreSQL replication lag
          description: "PostgreSQL replication lag is {{ $value }} seconds"

      # Redis down
      - alert: RedisDown
        expr: up{job="redis-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: Redis is down
          description: "Redis has been down for more than 1 minute"

      # Redis memory high
      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 90
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: Redis memory usage high
          description: "Redis memory usage is above 90%"

  # Security alerts
  - name: security
    rules:
      # High rate of failed authentication attempts
      - alert: HighFailedAuthAttempts
        expr: rate(auth_failures_total[5m]) > 10
        for: 2m
        labels:
          severity: warning
          team: security
        annotations:
          summary: High rate of failed authentication attempts
          description: "More than 10 failed authentication attempts per second"

      # Suspicious API access patterns
      - alert: SuspiciousAPIAccess
        expr: rate(http_requests_total{user_agent=~".*bot.*"}[5m]) > 100
        for: 5m
        labels:
          severity: info
          team: security
        annotations:
          summary: Suspicious API access pattern detected
          description: "High rate of bot-like API access detected"

  # Business metrics alerts
  - name: business
    rules:
      # Low user activity
      - alert: LowUserActivity
        expr: rate(active_users_total[1h]) < 10
        for: 1h
        labels:
          severity: info
          team: product
        annotations:
          summary: Low user activity
          description: "User activity is below 10 active users per hour"

      # High task completion rate
      - alert: HighTaskCompletionRate
        expr: rate(tasks_completed_total[5m]) > 100
        for: 10m
        labels:
          severity: info
          team: product
        annotations:
          summary: High task completion rate
          description: "Task completion rate is unusually high at {{ $value }} per minute"

      # Payment failures
      - alert: PaymentFailures
        expr: rate(payment_failures_total[5m]) / rate(payment_attempts_total[5m]) * 100 > 10
        for: 5m
        labels:
          severity: critical
          team: payments
        annotations:
          summary: High payment failure rate
          description: "Payment failure rate is above 10%"

  # Custom SLA/SLO alerts
  - name: sla
    rules:
      # API availability SLA breach
      - alert: APIAvailabilitySLABreach
        expr: (sum(rate(http_requests_total{job=~"labelmint-.*"}[5m])) - sum(rate(http_requests_total{job=~"labelmint-.*",status!~"5.."}[5m]))) / sum(rate(http_requests_total{job=~"labelmint-.*"}[5m])) * 100 < 99.9
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: API availability SLA breach
          description: "API availability has fallen below 99.9% SLA"

      # Response time SLO breach
      - alert: ResponseTimeSLOBreach
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=~"labelmint-.*"}[5m])) > 0.5
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: Response time SLO breach
          description: "95th percentile response time has exceeded 500ms SLO"